{
    "model": "NGPT",
    
    "dataset_path": "../datasets/pretrain/openwebtext.json",
    "tokenizer_path": "tokenizers/tokenizer64k",
    "max_length": 1024,
    "model_dim": 512,
    "num_heads": 8,
    "num_layers": 12,
    "dropout": 0,
    "batch_size": 2,
    "val_batch_size": 4,
    "n_batches_per_step": 64,
    "max_learning_rate": 6e-4,
    "min_learning_rate": 1e-6,
    "warmup_steps": 200,
    "total_steps": 100000,
    "validation_interval": 1000,
    "optimizer": "adamw",
    "betas": [0.9, 0.95],
    "weight_decay": 0.1,
    "bfloat16": true,
    
    "model_path": "",
    "optimizer_state_path": "",
    "checkpoint_step": 0,

    "log_file": "pretrain-gpt.log"
}